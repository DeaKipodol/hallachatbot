import asyncio
import os
from openai import OpenAI
from fastapi import APIRouter, HTTPException 
from pydantic import BaseModel
from dotenv import load_dotenv  
from fastapi.responses import StreamingResponse
from chatbotDirectory import chatbot
from chatbotDirectory.functioncalling import tools ,FunctionCalling
from chatbotDirectory.functioncalling import model
import json


class UserRequest(BaseModel):
    message: str
func_calling = FunctionCalling(
    model=model.basic,
    available_functions={
        # í•„ìš”ì‹œ ë‹¤ë¥¸ í•¨ìˆ˜ë„ ì—¬ê¸°ì— ì¶”ê°€
    }
)
router = APIRouter()

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)


# ì±„íŒ…
class Message(BaseModel):
    message: str

@router.post("/chat")
async def chat(message: Message):
    try:
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "user", "content": message.message}
            ],
            temperature=0.5,
            max_tokens=512,
        )
        answer = response.choices[0].message.content
        return {"response": answer.strip()}
    except Exception as e:
        print(f"OpenAI API error: {e}")  
        raise HTTPException(status_code=500, detail=str(e))
    
@router.post("/stream-chat")
async def stream_chat(user_input: UserRequest):
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìš°ì„  ì›ë³¸ ë¬¸ë§¥ì— ì¶”ê°€
    chatbot.add_user_message_in_context(user_input.message)
    
    chatbot.context[-1]['content'] += chatbot.instruction

    analyzed= func_calling.analyze(user_input.message, tools)

    temp_context = chatbot.to_openai_context().copy()
    

    for tool_call in analyzed:  # analyzedëŠ” list of function_call dicts
            if tool_call.type != "function_call":
                continue
            func_name = tool_call.name
            func_args = json.loads(tool_call.arguments)
            call_id = tool_call.call_id

            func_to_call = func_calling.available_functions.get(func_name)
            if not func_to_call:
                print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
                continue

            try:
               
                function_call_msg = {
                    "type": "function_call",  # ê³ ì •
                    "call_id": call_id,  # ë”•ì…”ë„ˆë¦¬ ë‚´ì— ìˆê±°ë‚˜ keyê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜
                    "name": func_name,
                    "arguments": tool_call.arguments  # dict -> JSON string
                }
                print(f"í•¨ìˆ˜ í˜¸ì¶œ ë©”ì‹œì§€: {function_call_msg}")
                if func_name == "search_internet":
                    # contextëŠ” ì´ë¯¸ run ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê³  ìˆìŒ
                   func_response = func_to_call(chat_context=chatbot.context[:], **func_args)
                else:
                   func_response = func_to_call(**func_args)

                temp_context.extend([
                    function_call_msg,
                {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": str(func_response)
                }
            ])
                print(temp_context)

            except Exception as e:
                print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

    # 4) í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ê°€ ë°˜ì˜ëœ temp_contextìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
    async def generate_with_tool():
        try:
            # stream=Trueë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            stream = client.responses.create(
            model=chatbot.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
              
            loading = True
            for event in stream:
                        match event.type:
                            case "response.created":
                                print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                                loading = True
                                # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                                yield "â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."
                                await asyncio.sleep(0)
                            case "response.output_text.delta":
                                if loading:
                                    print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")

                                    loading = False
                                # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                                yield f"{event.delta}"
                                await asyncio.sleep(0)
                            

                            case "response.in_progress":
                                print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")
                                yield "[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]"
                                yield "\n"

                            case "response.output_item.added":
                                if getattr(event.item, "type", None) == "reasoning":
                                    yield "[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]"
                                    yield "\n"
                                elif getattr(event.item, "type", None) == "message":
                                    yield "[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]"
                                    yield "\n"
                            #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                            case "response.output_item.done":
                                item = event.item
                                if item.type == "message" and item.role == "assistant":
                                    for part in item.content:
                                        if getattr(part, "type", None) == "output_text":
                                            completed_text= part.text
                            case "response.completed":
                                yield "\n"
                                #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                            case "response.failed":
                                print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                                yield "âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                            case "error":
                                print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                                yield "âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!"
                            case _:
                                yield "\n"
                                yield f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]"
        except Exception as e:
            yield f"\nStream Error: {str(e)}"
        finally:
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚˜ë©´ ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì—ë§Œ ë°˜ì˜í•˜ê³  ì„ì‹œë¬¸ë§¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                            # ê¸°ì¡´ clean ë°©ì‹ ìœ ì§€
            chatbot.add_response_stream( completed_text)
            
                        # ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì— ì €ì¥
    # 5) í•¨ìˆ˜ í˜¸ì¶œì´ ìˆì„ ë•ŒëŠ” ìœ„ì˜ generate_with_tool()ë¥¼ ì‚¬ìš©
    return StreamingResponse(generate_with_tool(), media_type="text/plain")