import asyncio
import os
from openai import OpenAI
from fastapi import APIRouter, HTTPException 
from pydantic import BaseModel
from dotenv import load_dotenv  
from fastapi.responses import StreamingResponse
from ..chatbotDirectory import chatbot
from ..chatbotDirectory.functioncalling import tools, FunctionCalling
from ..chatbotDirectory.functioncalling import model
from ..chatbotDirectory.chatbot import ChatbotStream
import json


# UserRequest í´ë˜ìŠ¤ì— language í•„ë“œ ì¶”ê°€
class UserRequest(BaseModel):
    message: str
    language: str = "KOR"  # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´ë¡œ ì„¤ì •

func_calling = FunctionCalling(
    model=model.basic,
    available_functions={
        # í•„ìš”ì‹œ ë‹¤ë¥¸ í•¨ìˆ˜ë„ ì—¬ê¸°ì— ì¶”ê°€
    }
)
router = APIRouter()

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

# ì˜ˆì‹œ: ì „ì—­ ë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ ë‚´ë¶€ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chatbot = ChatbotStream(
    model=model.advanced,
    system_role="""ë‹¹ì‹ ì€ í•™êµ ìƒí™œ, í•™ê³¼ ì •ë³´, í–‰ì‚¬ ë“± ì‚¬ìš©ìê°€ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì•„ëŠ” ë²”ìœ„ ì•ˆì—ì„œ ëŒ€ë‹µí•©ë‹ˆë‹¤. ë‹¨ ì ˆëŒ€ ê±°ì§“ë‚´ìš©ì„ ë§í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•„ëŠ” ë²”ìœ„ì—ì„œ ë§í•˜ê³  ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì¸ì •í•˜ì„¸ìš”.
    ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ê¸°ëŠ¥ì´ìˆìŠµë‹ˆë‹¤.
    ë‹¹ì‹ ì€ í•œë¼ëŒ€ ê³µì§€ì‚¬í•­ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ë‹¹ì‹ ì€ í•œë¼ëŒ€ í•™ì‹ë©”ë‰´ë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ë‹¹ì‹ ì€ í•œë¼ëŒ€ í•™ì‚¬ì¼ì •ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""",
    instruction="ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.",
    user="í•œë¼ëŒ€ ëŒ€í•™ìƒ",
    assistant="memmo"
)

# ì±„íŒ…
class Message(BaseModel):
    message: str

@router.post("/chat")
async def chat(message: Message):
    try:
        response = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "user", "content": message.message}
            ],

        )
        answer = response.choices[0].message.content
        return {"response": answer.strip()}
    except Exception as e:
        print(f"OpenAI API error: {e}")  
        raise HTTPException(status_code=500, detail=str(e))
    
@router.post("/stream-chat")
async def stream_chat(user_input: UserRequest):
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì›ë³¸ ë¬¸ë§¥ì— ì¶”ê°€
    chatbot.add_user_message_in_context(user_input.message)

    # 2) ì–¸ì–´ ì§€ì¹¨ ì¶”ê°€
    instruction_map = {
        "KOR": "í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ë”°ëœ»í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.",
        "ENG": "Please respond kindly in English.",
        "VI": "Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch nháº¹ nhÃ ng.",
        "JPN": "æ—¥æœ¬èªã§ä¸å¯§ã«æ¸©ã‹ãç­”ãˆã¦ãã ã•ã„ã€‚",
        "CHN": "è¯·ç”¨ä¸­æ–‡äº²åˆ‡åœ°å›ç­”ã€‚",
    }
    instruction = instruction_map.get(user_input.language, instruction_map["KOR"])
    chatbot.context[-1]["content"] += " " + instruction

    # 3) RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
    rag_ctx = chatbot.get_rag_context(user_input.message)
    has_rag = bool(rag_ctx and rag_ctx.strip())

    # 4) í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„ ë° ì‹¤í–‰
    analyzed = func_calling.analyze(user_input.message, tools)
    func_msgs: list[dict] = []
    func_outputs: list[str] = []

    for tool_call in analyzed:
        if getattr(tool_call, "type", None) != "function_call":
            continue
        func_name = tool_call.name
        func_args = json.loads(tool_call.arguments)
        call_id = tool_call.call_id

        func_to_call = func_calling.available_functions.get(func_name)
        if not func_to_call:
            print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
            continue

        try:
            # ì•ˆì „ ê¸°ë³¸ê°’ ë³´ê°•
            if func_name == "get_halla_cafeteria_menu":
                func_args.setdefault("date", "ì˜¤ëŠ˜")
                func_args.setdefault("meal", "ì¤‘ì‹")

            func_response = (
                func_to_call(chat_context=chatbot.context[:], **func_args)
                if func_name == "search_internet"
                else func_to_call(**func_args)
            )

            func_msgs.extend([
                {
                    "type": "function_call",
                    "call_id": call_id,
                    "name": func_name,
                    "arguments": tool_call.arguments,
                },
                {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": str(func_response),
                },
            ])
            func_outputs.append(str(func_response))
        except Exception as e:
            print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

    has_funcs = len(func_outputs) > 0

    # 4-1) í•™ì‹/ì‹ë‹¨ ì§ˆì˜ ë³´ê°• í˜¸ì¶œ (LLM ëˆ„ë½ ëŒ€ë¹„ + ê²°ê³¼ ìš”ì•½ system ì£¼ì…)
    lowered = user_input.message.lower()
    cafeteria_keywords = any(k in lowered for k in ["í•™ì‹", "ì‹ë‹¨", "ì ì‹¬", "ì €ë…", "ë©”ë‰´", "ì¡°ì‹", "ì„ì‹"])
    already_called_cafeteria = any(m.get("name") == "get_halla_cafeteria_menu" for m in func_msgs if m.get("type") == "function_call")


    if cafeteria_keywords and not already_called_cafeteria:
        try:
            print("[DEBUG] Cafeteria fallback engaged (missing function call)")
            meal_pref = "ì¤‘ì‹"
            if any(x in lowered for x in ["ì¡°ì‹", "ì•„ì¹¨"]):
                meal_pref = "ì¡°ì‹"
            elif any(x in lowered for x in ["ì„ì‹", "ì €ë…"]):
                meal_pref = "ì„ì‹"
            date_pref = "ì˜¤ëŠ˜"
            if "ë‚´ì¼" in lowered:
                date_pref = "ë‚´ì¼"
            else:
                import re as _re
                m = _re.search(r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})", user_input.message)
                if m:
                    date_pref = m.group(1)
            caf_args = {"date": date_pref, "meal": meal_pref}
            get_cafeteria_fn = func_calling.available_functions.get("get_halla_cafeteria_menu")
            if not get_cafeteria_fn:
                raise RuntimeError("get_halla_cafeteria_menu not registered")
            caf_out = get_cafeteria_fn(**caf_args)
            call_id = "cafeteria_auto"
            func_msgs.extend([
                {"type": "function_call", "call_id": call_id, "name": "get_halla_cafeteria_menu", "arguments": json.dumps(caf_args, ensure_ascii=False)},
                {"type": "function_call_output", "call_id": call_id, "output": str(caf_out)},
            ])
            func_outputs.append(str(caf_out))
            has_funcs = True
            # ê°„ë‹¨ ìš”ì•½ ë¸”ë¡ (LLM í˜¸ì¶œ ì—†ì´ ê·œì¹™ ê¸°ë°˜ ì¶•ì•½)
            first_lines = "\n".join([ln for ln in str(caf_out).splitlines()[:8]])
            cafeteria_summary_block = f"<í•™ì‹ìš”ì•½>ìš”ì²­ì¼ì={date_pref}, ì‹ì‚¬={meal_pref}\n{first_lines}</í•™ì‹ìš”ì•½>"
        except Exception as e:
            print(f"[ë³´ê°• í˜¸ì¶œ ì‹¤íŒ¨] get_halla_cafeteria_menu: {e}")

    # 5) ìµœì¢… ìŠ¤íŠ¸ë¦¬ë°ì— ì‚¬ìš©í•  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    base_context = chatbot.to_openai_context(chatbot.context[:])
    temp_context = base_context[:]

    # ì´í›„ í•˜ë‚˜ì˜ system ë©”ì‹œì§€ë¡œ í•©ì¹  ì„¹ì…˜ì„ ìˆ˜ì§‘
    sections: list[str] = []
    query_guidance = (
        f"ì´ê²ƒì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤: {user_input.message}\n"
        "ë‹¤ìŒ ì •ë³´ë¥¼ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë‹µì— ë§ê²Œ í†µí•©í•´ ì „ë‹¬í•˜ì„¸ìš”.\n"
        "- í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜\n- ê¸°ì–µê²€ìƒ‰ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜ / í•¨ìˆ˜ í˜¸ì¶œ ì¡´ì¬ ìì²´ëŠ” ì–¸ê¸‰ ê¸ˆì§€"
    )
    sections.append("[ì‚¬ìš©ìì¿¼ë¦¬ì§€ì¹¨]\n" + query_guidance)
    sections.append("[ì¼ë°˜ì§€ì¹¨]\n" + chatbot.instruction)

    if has_rag:
        # 5-1) ê¸°ì–µê²€ìƒ‰ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë„£ì§€ ì•Šê³ , ë¨¼ì € LLMìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ê°€ê³µ/ìš”ì•½
        def _sanitize_text(txt: str) -> str:
            # ì œì–´ë¬¸ì ì œê±° ë° íƒœê·¸ ì¶©ëŒ ë°©ì§€
            if not isinstance(txt, str):
                txt = str(txt)
            # ê°„ë‹¨ ì œì–´ë¬¸ì í•„í„°ë§ (LF, TAB ì œì™¸)
            txt = ''.join(ch for ch in txt if ch in ('\n', '\t') or (ord(ch) >= 32 and ch != 127))
            # íƒœê·¸ ì¡°ê¸° ì¢…ë£Œ ë°©ì§€
            txt = txt.replace("</ê¸°ì–µê²€ìƒ‰>", "[/ê¸°ì–µê²€ìƒ‰]")
            # ê³¼ë„í•œ ê¸¸ì´ í´ë¨í”„ (í•„ìš”ì‹œ ì¡°ì •)
            max_len = 12000
            return txt[:max_len]

        sanitized_rag = _sanitize_text(rag_ctx)

        condense_prompt = [
            {
                "role": "system",
                "content": (
                    f"""
ë‹¹ì‹ ì€ ê¸´ ê·œì •/ì„¸ì¹™ ë¬¸ì„œ ë¬¶ìŒì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œÂ·í‘œì‹œí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ê·œì¹™:
1) ì›ë¬¸ ì „ì²´ëŠ” <ê¸°ì–µê²€ìƒ‰> íƒœê·¸ ì•ˆì— ìˆìŠµë‹ˆë‹¤.
2) ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ê·¼ê±° ë¬¸ì¥/ë‹¨ë½ë§Œ <ë°˜ì˜>...</ë°˜ì˜> íƒœê·¸ ì•ˆì— ê·¸ëŒ€ë¡œ(ê°€ëŠ¥í•œ ìˆ˜ì • ìµœì†Œí™”) ë„£ìœ¼ì„¸ìš”.
3) ê·¼ê±°ë¥¼ ì°¾ê¸° ì–´ë µê±°ë‚˜ ëª¨í˜¸í•˜ë©´ <ë°˜ì˜>ê´€ë ¨ ê·¼ê±° ì—†ìŒ</ë°˜ì˜> ë§Œ ë„£ìœ¼ì„¸ìš”.
4) ì›ë¬¸ êµ¬ì¡°(ì¡°/í•­/í˜¸ ë²ˆí˜¸)ëŠ” ìœ ì§€í•˜ê³  ë¶ˆí•„ìš”í•œ ìš”ì•½ì€ í•˜ì§€ ë§ˆì„¸ìš”.
5) ì›ë¬¸ ë°– ì¶”ë¡ /ì°½ì‘ ê¸ˆì§€.

ì‚¬ìš©ì ì§ˆë¬¸: {user_input.message}
<ê¸°ì–µê²€ìƒ‰>{sanitized_rag}</ê¸°ì–µê²€ìƒ‰>
"""
                ),
            }
        ]

        # ë””ë²„ê·¸: condense_promptì™€ rag_ctx ì¶œë ¥
        print("==== [DEBUG] condense_prompt ====")
        for item in condense_prompt:
            print(item)
        print("==== [DEBUG] rag_ctx ====")
        print(rag_ctx)

        try:
            condensed = client.responses.create(
                model=model.advanced,
                input=condense_prompt,
                text={"format": {"type": "text"}},
            ).output_text.strip()
            print("==== [DEBUG] condensed ====")
            print(condensed)
        except Exception as _e:
            # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ì„ ì§§ê²Œ ì˜ë¼ ì‚¬ìš©
            print(f"[DEBUG] ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {_e}")
            condensed = sanitized_rag[:3000]

        rag_guidance = (
            "ê¸°ì–µê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. <ë°˜ì˜> </ë°˜ì˜> íƒœê·¸ ë‚´ë¶€ ë‚´ìš©ì„ ë³´ê³  ì‚¬ìš©ìì˜ ì›í•˜ëŠ” ì¿¼ë¦¬ì— ë§ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”. "
            "<ê¸°ì–µê²€ìƒ‰></ê¸°ì–µê²€ìƒ‰> íƒœê·¸ëŠ” ì°¸ì¡°ìš©ì´ë©° íƒœê·¸ ë°– ì„ì˜ ì°½ì‘ ê¸ˆì§€"
        )
        sections.append("[ê¸°ì–µê²€ìƒ‰ì§€ì¹¨]\n" + rag_guidance)
        sections.append("[ê¸°ì–µê²€ìƒ‰]\n<ê¸°ì–µê²€ìƒ‰>\n" + condensed + "\n</ê¸°ì–µê²€ìƒ‰>")

    if has_funcs:
        # í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ ë¬¸ìì—´ êµ¬ì„±
        formatted_blocks = []
        # func_msgsëŠ” [call, output, call, output, ...] êµ¬ì¡°ì´ë¯€ë¡œ 2ê°œì”© ë¬¶ì–´ ì²˜ë¦¬
        try:
            for i in range(0, len(func_msgs), 2):
                call = func_msgs[i]
                if i + 1 < len(func_msgs):
                    output = func_msgs[i + 1]
                else:
                    output = {"output": "(ì¶œë ¥ ëˆ„ë½)"}
                if call.get("type") != "function_call":
                    continue
                name = call.get("name")
                args = call.get("arguments")
                out_text = output.get("output", "") if isinstance(output, dict) else str(output)
                # ë„ˆë¬´ ê¸´ ì¶œë ¥ì€ ì˜ë¼ëƒ„ (ì•ˆì „)
                max_len = 4000
                if len(out_text) > max_len:
                    out_text = out_text[:max_len] + "...<truncated>"
                formatted_blocks.append(f"<function name='{name}' args='{args}'>\n{out_text}\n</function>")
        except Exception as _fmt_e:
            print(f"[DEBUG] function result formatting error: {_fmt_e}")
        functions_block = "\n".join(formatted_blocks) if formatted_blocks else "(í•¨ìˆ˜ ê²°ê³¼ í¬ë§· ì—†ìŒ)"

        # í•™ì‹ ë³´ê°• ìš”ì•½ ë¸”ë¡ì´ ìˆë‹¤ë©´ í¬í•¨
        try:
            if 'cafeteria_summary_block' in locals() and cafeteria_summary_block:
                functions_block += f"\n{cafeteria_summary_block}"
        except Exception:
            pass

        func_guidance = (
            "ë‹¤ìŒì€ í•¨ìˆ˜(ê²€ìƒ‰/ë©”ë‰´ ë“±) í˜¸ì¶œ ê²°ê³¼ì…ë‹ˆë‹¤. <í•¨ìˆ˜ê²°ê³¼> íƒœê·¸ ë‚´ë¶€ ë‚´ìš©ë§Œ ì‚¬ì‹¤ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ê³  "
            "'í•¨ìˆ˜ í˜¸ì¶œ'ì´ë¼ëŠ” í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ë©° ê±°ì§“ ì •ë³´ ìƒì„± ê¸ˆì§€"
        )
        sections.append("[í•¨ìˆ˜ê²°ê³¼ì§€ì¹¨]\n" + func_guidance)
        sections.append("[í•¨ìˆ˜ê²°ê³¼]\n<í•¨ìˆ˜ê²°ê³¼>\n" + functions_block + "\n</í•¨ìˆ˜ê²°ê³¼>")
       

    if has_rag and has_funcs:
        merge_instruction = (
            "ìœ„ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°(<ê¸°ì–µê²€ìƒ‰>)ì™€ í•¨ìˆ˜/ê²€ìƒ‰ ê²°ê³¼(<í•¨ìˆ˜ê²°ê³¼>)ë¥¼ ëŒ€ì¡°í•˜ì—¬ ëª¨ìˆœ ì—†ê²Œ í•µì‹¬ ë‹µ ë¨¼ì €, í•„ìš”í•œ ê·¼ê±° ì¶•ì•½ ì œì‹œ. ê·¼ê±° ì—†ìœ¼ë©´ ëª…ì‹œ."
        )
        sections.append("[í†µí•©ì§€ì¹¨]\n" + merge_instruction)

    # ë‹¨ì¼ system ë©”ì‹œì§€ë¡œ ë³‘í•© ì¶”ê°€
    temp_context.append({
        "role": "system",
        "content": "\n\n".join(sections)
    })

    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•´ ì¼ë°˜ ì‘ë‹µ
    context_to_stream = temp_context if (has_rag or has_funcs) else base_context

    # 6) ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë° ìµœì¢… ë¬¸ë§¥ ì €ì¥
    async def generate_with_tool():
        completed_text = ""
        try:
            stream = client.responses.create(
                model=chatbot.model,
                input=context_to_stream,
                top_p=1,
                stream=True,
                text={"format": {"type": "text"}},
            )

            loading = True
            for event in stream:
                match event.type:
                    case "response.created":
                        loading = True
                        yield "â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."
                        await asyncio.sleep(0)
                    case "response.output_text.delta":
                        if loading:
                            yield "\n[ï¿½ ì‘ë‹µ ì‹œì‘ë¨ â†“]"
                            loading = False
                        yield f"{event.delta}"
                        await asyncio.sleep(0)
                    case "response.in_progress":
                        yield "\n[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]\n"
                    case "response.output_item.done":
                        item = event.item
                        if item.type == "message" and item.role == "assistant":
                            for part in item.content:
                                if getattr(part, "type", None) == "output_text":
                                    completed_text = part.text
                    case "response.completed":
                        yield "\n"
                    case "response.failed":
                        yield "âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                    case "error":
                        yield "âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!"
                    case _:
                        yield f"\n[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]"
        except Exception as e:
            yield f"\nStream Error: {str(e)}"
        finally:
            if completed_text:
                chatbot.add_response_stream(completed_text)
            print(context_to_stream)

    return StreamingResponse(generate_with_tool(), media_type="text/plain")

