import math
import os
import sys
from pathlib import Path

# ì‹¤í–‰ ë°©ì‹ê³¼ ë¬´ê´€í•˜ê²Œ íŒ¨í‚¤ì§€ ë£¨íŠ¸ë¥¼ ì¸ì§€ì‹œí‚¤ê¸° ìœ„í•œ ë¶€íŠ¸ìŠ¤íŠ¸ë©
# (ì˜ˆ: `python chatbotDirectory/chatbot.py`ë¡œ ì§ì ‘ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ëŒ€ë¹„)
_ROOT = Path(__file__).resolve().parent.parent
if str(_ROOT) not in sys.path:
    sys.path.insert(0, str(_ROOT)) 
# ìƒëŒ€ ì„í¬íŠ¸ ëŒ€ì‹  ì ˆëŒ€ ì„í¬íŠ¸ ì‚¬ìš©
from .common import model, client
from .functioncalling import FunctionCalling, tools
from .rag import RagService
import json

class ChatbotStream:
    def __init__(self, model,system_role,instruction,**kwargs):
        """
        ì´ˆê¸°í™”:
          - context ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì‹œìŠ¤í…œ ì—­í•  ì„¤ì •
          - sub_contexts ì„œë¸Œ ëŒ€í™”ë°© ë¬¸ë§¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ {í•„ë“œì´ë¦„,ë¬¸ë§¥,ìš”ì•½,ì§ˆë¬¸} êµ¬ì„±
          - current_field = í˜„ì¬ ëŒ€í™”ë°© ì¶”ì  (ê¸°ë³¸ê°’: ë©”ì¸ ëŒ€í™”ë°©
          - openai.api_key ì„¤ì •
          - ì‚¬ìš©í•  ëª¨ë¸ëª… ì €ì¥
          - ì‚¬ìš©ì ì´ë¦„
        """
        self.context = [{"role": "system","content": system_role}]
               
        self.current_field = "main"
        
        self.model = model
        self.instruction=instruction

        self.max_token_size = 16 * 1024 #ìµœëŒ€ í† í°ì´ìƒì„ ì“°ë©´ ì˜¤ë¥˜ê°€ë°œìƒ ë”°ë¼ì„œ í† í° ìš©ëŸ‰ê´€ë¦¬ê°€ í•„ìš”.
        self.available_token_rate = 0.9#ìµœëŒ€í† í°ì˜ 90%ë§Œ ì“°ê² ë‹¤.

        # ë””ë²„ê·¸ í”Œë˜ê·¸ (í™˜ê²½ë³€ìˆ˜ RAG_DEBUGë¡œ ì œì–´: ê¸°ë³¸ í™œì„±í™”)
        self.debug = os.getenv("RAG_DEBUG", "1") not in ("0", "false", "False")

        # Phase 2: ëª¨ë“ˆí˜• RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™”
        self.rag_service = RagService(debug_fn=self._dbg)
        self._last_rag_result = None

    def _dbg(self, msg: str):
        """ì‘ì€ ë””ë²„ê·¸ í—¬í¼: RAG ê´€ë ¨ ë‚´ë¶€ ìƒíƒœë¥¼ ë³´ê¸° ì‰½ê²Œ ì¶œë ¥."""
        if self.debug:
            print(f"[RAG-DEBUG] {msg}")

    def add_user_message_in_context(self, message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€:
          - ì‚¬ìš©ìê°€ ì…ë ¥í•œ messageë¥¼ contextì— user ì—­í• ë¡œ ì¶”ê°€
        """
        assistant_message = {
            "role": "user",
            "content": message,
        }
        if self.current_field == "main":
            self.context.append(assistant_message)

    #ì „ì†¡ë¶€
    def _send_request_Stream(self,temp_context=None):
        
        completed_text = ""

        if temp_context is None:
           current_context = self.get_current_context()
           openai_context = self.to_openai_context(current_context)
           stream = client.responses.create(
            model=self.model,
            input=openai_context,  
            top_p=1,
            stream=True,
            
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        else:  
           stream = client.responses.create(
            model=self.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        
        loading = True  # deltaê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ë¡œë”© ì¤‘ ìƒíƒœ ìœ ì§€       
        for event in stream:
            #print(f"event: {event}")
            match event.type:
                case "response.created":
                    print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                    loading = True
                    # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                    print("â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...")
                    
                case "response.output_text.delta":
                    if loading:
                        print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")
                        loading = False
                    # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                    print(event.delta, end="", flush=True)
                 

                case "response.in_progress":
                    print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")

                case "response.output_item.added":
                    if getattr(event.item, "type", None) == "reasoning":
                        print("[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]")
                    elif getattr(event.item, "type", None) == "message":
                        print("[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]")
                #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                case "response.output_item.done":
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text= part.text
                case "response.completed":
                    print("\n")
                    #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                case "response.failed":
                    print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                case "error":
                    print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                case _:
                    
                    print(f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]")
        return completed_text
  
  
    def send_request_Stream(self):
      self.context[-1]['content']+=self.instruction
      return self._send_request_Stream()
#ì±—ë´‡ì— ë§ê²Œ ë¬¸ë§¥ íŒŒì‹±
    def add_response(self, response):
        response_message = {
            "role" : response['choices'][0]['message']["role"],
            "content" : response['choices'][0]['message']["content"],
            
        }
        self.context.append(response_message)

    def add_response_stream(self, response):
            """
                ì±—ë´‡ ì‘ë‹µì„ í˜„ì¬ ëŒ€í™”ë°©ì˜ ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                
                Args:
                    response (str): ì±—ë´‡ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸.
                """
            assistant_message = {
            "role": "assistant",
            "content": response,
           
        }
            self.context.append(assistant_message)

    def get_response(self, response_text: str):
        """
        ì‘ë‹µë‚´ìš©ë°˜í™˜:
          - ë©”ì‹œì§€ë¥¼ ì½˜ì†”(ë˜ëŠ” UI) ì¶œë ¥ í›„, ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        print(response_text['choices'][0]['message']['content'])
        return response_text
#ë§ˆì§€ë§‰ ì§€ì¹¨ì œê±°
    def clean_context(self):
        '''
        1.contextë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¶€í„° ì²˜ìŒê¹Œì§€ ìˆœíšŒí•œë‹¤
        2."instruction:\n"ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆˆë‹¤..ì²«userì„ ì°¾ìœ¼ë©´ ì•„ë˜ ê³¼ì •ì„ ì§„í–‰í•œë‹¤,
        3.ì²« ë²ˆì§¸ ë¶€ë¶„ [0]ë§Œ ê°€ì ¸ì˜¨ë‹¤. (ì¦‰, "instruction:\n" ì´ì „ì˜ ë¬¸ìì—´ë§Œ ë‚¨ê¸´ë‹¤.)
        4.strip()ì„ ì ìš©í•˜ì—¬ ì•ë’¤ì˜ ê³µë°±ì´ë‚˜ ê°œí–‰ ë¬¸ìë¥¼ ì œê±°í•œë‹¤.
        '''
        for idx in reversed(range(len(self.context))):
            if self.context[idx]['role']=='user':
                self.context[idx]["content"]=self.context[idx]['content'].split('instruction:\n')[0].strip()
                break
#ì§ˆì˜ì‘ë‹µ í† í° ê´€ë¦¬
    def handle_token_limit(self, response):
        # ëˆ„ì  í† í° ìˆ˜ê°€ ì„ê³„ì ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì–´í•œë‹¤.
        try:
            current_usage_rate = response['usage']['total_tokens'] / self.max_token_size
            exceeded_token_rate = current_usage_rate - self.available_token_rate
            if exceeded_token_rate > 0:
                remove_size = math.ceil(len(self.context) / 10)
                self.context = [self.context[0]] + self.context[remove_size+1:]
        except Exception as e:
            print(f"handle_token_limit exception:{e}")
            
    def to_openai_context(self, context):
        return [{"role":v["role"], "content":v["content"]} for v in context]

    def get_current_context(self):
        """í˜„ì¬ ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.context

    def get_rag_context(self, user_question: str):
        """RAG ì»¨í…ìŠ¤íŠ¸ë§Œ ì¤€ë¹„í•˜ì—¬ ë°˜í™˜ (ì—†ìœ¼ë©´ None). ì—¬ê¸°ì„œë¶€í„° ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì·¨ê¸‰."""
        result = self.rag_service.build_context(user_question)
        self._last_rag_result = result
        return result.context_text

    @property
    def last_rag_result(self):
        """ìµœê·¼ RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤ (ì—†ìœ¼ë©´ None)."""
        return self._last_rag_result or self.rag_service.last_result
    def get_response_from_db_only(self, user_question: str):
        self._dbg("get_response_from_db_only: start")
        rag_result = self.rag_service.build_context(user_question)
        rag_context = rag_result.context_text
        if rag_context is None:
            self._dbg("ê¸°ì–µê²€ìƒ‰ ì•„ë‹˜")
            return False

        # LLM í˜¸ì¶œí•  context êµ¬ì„±: system ë©”ì‹œì§€ + DB ë‚´ìš©(system role) + user ì§ˆë¬¸
        context = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ì‚¬ ê·œì • ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "system", "content": rag_context},
            {"role": "user", "content": user_question},
        ]
        self._dbg(
            f"get_response_from_db_only: messages=[system, system(ctx:{len(rag_context)} chars), user] model={self.model}"
        )

        return self._send_request_Stream(temp_context=self.to_openai_context(context))
 


