import math
import os
import json
from typing import Any, Dict, List, Optional, AsyncGenerator

# ìƒˆë¡œìš´ import ê²½ë¡œ
from app.ai.chatbot.config import model, client
from app.ai.chatbot.metadata import FunctionCallMetadata, RagMetadata, ChatMetadata
from app.ai.functions import FunctionCalling, tools
from app.ai.rag.service import RagService

class ChatbotStream:
    def __init__(self, model,system_role,instruction,**kwargs):
        """
        ì´ˆê¸°í™”:
          - context ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì‹œìŠ¤í…œ ì—­í•  ì„¤ì •
          - sub_contexts ì„œë¸Œ ëŒ€í™”ë°© ë¬¸ë§¥ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ {í•„ë“œì´ë¦„,ë¬¸ë§¥,ìš”ì•½,ì§ˆë¬¸} êµ¬ì„±
          - current_field = í˜„ì¬ ëŒ€í™”ë°© ì¶”ì  (ê¸°ë³¸ê°’: ë©”ì¸ ëŒ€í™”ë°©
          - openai.api_key ì„¤ì •
          - ì‚¬ìš©í•  ëª¨ë¸ëª… ì €ì¥
          - ì‚¬ìš©ì ì´ë¦„
        """
        self.context = [{"role": "system","content": system_role}]
               
        self.current_field = "main"
        
        self.model = model
        self.instruction=instruction

        self.max_token_size = 16 * 1024 #ìµœëŒ€ í† í°ì´ìƒì„ ì“°ë©´ ì˜¤ë¥˜ê°€ë°œìƒ ë”°ë¼ì„œ í† í° ìš©ëŸ‰ê´€ë¦¬ê°€ í•„ìš”.
        self.available_token_rate = 0.9#ìµœëŒ€í† í°ì˜ 90%ë§Œ ì“°ê² ë‹¤.

        # ë””ë²„ê·¸ í”Œë˜ê·¸ (í™˜ê²½ë³€ìˆ˜ RAG_DEBUGë¡œ ì œì–´: ê¸°ë³¸ í™œì„±í™”)
        self.debug = os.getenv("RAG_DEBUG", "1") not in ("0", "false", "False")

        # Phase 2: ëª¨ë“ˆí˜• RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™”
        self.rag_service = RagService(debug_fn=self._dbg)
        self._last_rag_result = None
        self._last_web_status: Optional[str] = None
        
        # Phase 2: í•¨ìˆ˜ í˜¸ì¶œ ê´€ë ¨ ì¸ìŠ¤í„´ìŠ¤í™”
        self.func_calling = FunctionCalling(model=model)
        self.tools = tools
        self.available_functions = self.func_calling.available_functions if hasattr(self.func_calling, 'available_functions') else {}

    def _dbg(self, msg: str):
        """ì‘ì€ ë””ë²„ê·¸ í—¬í¼: RAG ê´€ë ¨ ë‚´ë¶€ ìƒíƒœë¥¼ ë³´ê¸° ì‰½ê²Œ ì¶œë ¥."""
        if self.debug:
            print(f"[RAG-DEBUG] {msg}")

    def add_user_message_in_context(self, message: str):
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€:
          - ì‚¬ìš©ìê°€ ì…ë ¥í•œ messageë¥¼ contextì— user ì—­í• ë¡œ ì¶”ê°€
        """
        assistant_message = {
            "role": "user",
            "content": message,
        }
        if self.current_field == "main":
            self.context.append(assistant_message)

    #ì „ì†¡ë¶€
    def _send_request_Stream(self,temp_context=None):
        
        completed_text = ""

        if temp_context is None:
           current_context = self.get_current_context()
           openai_context = self.to_openai_context(current_context)
           stream = client.responses.create(
            model=self.model,
            input=openai_context,  
            top_p=1,
            stream=True,
            
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        else:  
           stream = client.responses.create(
            model=self.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
        
        loading = True  # deltaê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ë¡œë”© ì¤‘ ìƒíƒœ ìœ ì§€       
        for event in stream:
            #print(f"event: {event}")
            match event.type:
                case "response.created":
                    print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                    loading = True
                    # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                    print("â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...")
                    
                case "response.output_text.delta":
                    if loading:
                        print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")
                        loading = False
                    # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                    print(event.delta, end="", flush=True)
                 

                case "response.in_progress":
                    print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")

                case "response.output_item.added":
                    if getattr(event.item, "type", None) == "reasoning":
                        print("[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]")
                    elif getattr(event.item, "type", None) == "message":
                        print("[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]")
                #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                case "response.output_item.done":
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text= part.text
                case "response.completed":
                    print("\n")
                    #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                case "response.failed":
                    print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                case "error":
                    print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                case _:
                    
                    print(f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]")
        return completed_text
  
  
    def send_request_Stream(self):
      self.context[-1]['content']+=self.instruction
      return self._send_request_Stream()
    #ì±—ë´‡ì— ë§ê²Œ ë¬¸ë§¥ íŒŒì‹±
    def add_response(self, response):
        response_message = {
            "role" : response['choices'][0]['message']["role"],
            "content" : response['choices'][0]['message']["content"],            
        }
        self.context.append(response_message)

    def add_response_stream(self, response):
            """
                ì±—ë´‡ ì‘ë‹µì„ í˜„ì¬ ëŒ€í™”ë°©ì˜ ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.
                
                Args:
                    response (str): ì±—ë´‡ì´ ìƒì„±í•œ ì‘ë‹µ í…ìŠ¤íŠ¸.
                """
            assistant_message = {
            "role": "assistant",
            "content": response,
           
        }
            self.context.append(assistant_message)

    def get_response(self, response_text: str):
        """
        ì‘ë‹µë‚´ìš©ë°˜í™˜:
          - ë©”ì‹œì§€ë¥¼ ì½˜ì†”(ë˜ëŠ” UI) ì¶œë ¥ í›„, ê·¸ëŒ€ë¡œ ë°˜í™˜
        """
        print(response_text['choices'][0]['message']['content'])
        return response_text
#ë§ˆì§€ë§‰ ì§€ì¹¨ì œê±°
    def clean_context(self):
        '''
        1.contextë¦¬ìŠ¤íŠ¸ì— ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¶€í„° ì²˜ìŒê¹Œì§€ ìˆœíšŒí•œë‹¤
        2."instruction:\n"ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆˆë‹¤..ì²«userì„ ì°¾ìœ¼ë©´ ì•„ë˜ ê³¼ì •ì„ ì§„í–‰í•œë‹¤,
        3.ì²« ë²ˆì§¸ ë¶€ë¶„ [0]ë§Œ ê°€ì ¸ì˜¨ë‹¤. (ì¦‰, "instruction:\n" ì´ì „ì˜ ë¬¸ìì—´ë§Œ ë‚¨ê¸´ë‹¤.)
        4.strip()ì„ ì ìš©í•˜ì—¬ ì•ë’¤ì˜ ê³µë°±ì´ë‚˜ ê°œí–‰ ë¬¸ìë¥¼ ì œê±°í•œë‹¤.
        '''
        for idx in reversed(range(len(self.context))):
            if self.context[idx]['role']=='user':
                self.context[idx]["content"]=self.context[idx]['content'].split('instruction:\n')[0].strip()
                break
#ì§ˆì˜ì‘ë‹µ í† í° ê´€ë¦¬
    def handle_token_limit(self, response):
        # ëˆ„ì  í† í° ìˆ˜ê°€ ì„ê³„ì ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì–´í•œë‹¤.
        try:
            current_usage_rate = response['usage']['total_tokens'] / self.max_token_size
            exceeded_token_rate = current_usage_rate - self.available_token_rate
            if exceeded_token_rate > 0:
                remove_size = math.ceil(len(self.context) / 10)
                self.context = [self.context[0]] + self.context[remove_size+1:]
        except Exception as e:
            print(f"handle_token_limit exception:{e}")
            
    def to_openai_context(self, context):
        return [{"role":v["role"], "content":v["content"]} for v in context]

    def get_current_context(self):
        """í˜„ì¬ ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        return self.context

    def get_rag_context(self, user_question: str):
        """RAG ì»¨í…ìŠ¤íŠ¸ë§Œ ì¤€ë¹„í•˜ì—¬ ë°˜í™˜ (ì—†ìœ¼ë©´ None). ì—¬ê¸°ì„œë¶€í„° ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì·¨ê¸‰."""
        result = self.rag_service.retrieve_context(user_question)
        self._last_rag_result = result
        return result.merged_documents_text

    @property
    def last_rag_result(self):
        """ìµœê·¼ RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤ (ì—†ìœ¼ë©´ None)."""
        return self._last_rag_result or self.rag_service.last_result
    def get_response_from_db_only(self, user_question: str):
        self._dbg("get_response_from_db_only: start")
        rag_result = self.rag_service.retrieve_context(user_question)
        rag_context = rag_result.merged_documents_text
        if rag_context is None:
            self._dbg("ê¸°ì–µê²€ìƒ‰ ì•„ë‹˜")
            return False

        # LLM í˜¸ì¶œí•  context êµ¬ì„±: system ë©”ì‹œì§€ + DB ë‚´ìš©(system role) + user ì§ˆë¬¸
        context = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ í•™ì‚¬ ê·œì • ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "system", "content": rag_context},
            {"role": "user", "content": user_question},
        ]
        self._dbg(
            f"get_response_from_db_only: messages=[system, system(ctx:{len(rag_context)} chars), user] model={self.model}"
        )

        return self._send_request_Stream(temp_context=self.to_openai_context(context))

    # ==========================================
    # Phase 2: í—¬í¼ ë©”ì„œë“œ êµ¬í˜„ (routes.py â†’ stream.py)
    # ==========================================

    def _get_language_instruction(self, language: str) -> str:
        """
        ì–¸ì–´ ì½”ë“œì— ë”°ë¥¸ ì‘ë‹µ ì§€ì¹¨ ë°˜í™˜
        
        Args:
            language: ì–¸ì–´ ì½”ë“œ (KOR, ENG, VI, JPN, CHN, UZB, MNG, IDN)
        
        Returns:
            str: í•´ë‹¹ ì–¸ì–´ì— ë§ëŠ” ì‘ë‹µ ì§€ì¹¨
        """
        instruction_map = {
            "KOR": "í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ë”°ëœ»í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.",
            "ENG": "Please respond kindly in English.",
            "VI": "Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch nháº¹ nhÃ ng.",
            "JPN": "æ—¥æœ¬èªã§ä¸å¯§ã«æ¸©ã‹ãç­”ãˆã¦ãã ã•ã„ã€‚",
            "CHN": "è¯·ç”¨ä¸­æ–‡äº²åˆ‡åœ°å›ç­”ã€‚",
            "UZB": "Iltimos, o'zbek tilida samimiy va hurmat bilan javob bering.",
            "MNG": "ĞœĞ¾Ğ½Ğ³Ğ¾Ğ» Ñ…ÑĞ»ÑÑÑ€ ÑĞµĞ»Ğ´ÑĞ³, Ğ´ÑƒĞ»Ğ°Ğ°Ñ…Ğ°Ğ½ Ñ…Ğ°Ñ€Ğ¸ÑƒĞ»Ğ½Ğ° ÑƒÑƒ.",
            "IDN": "Tolong jawab dengan ramah dan hangat dalam bahasa Indonesia."
        }
        return instruction_map.get(language, instruction_map["KOR"])

    async def _condense_rag_context(self, user_question: str, raw_context: str) -> str:
        """
        ê¸´ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ê²Œ ìš”ì•½
        
        - í‘œ/ë²ˆí˜¸ì¡°í•­ì˜ ì „ì²´ ë§¥ë½ í¬í•¨ (ìµœì†Œ 15ì¤„)
        - ì£¼ì„(ì£¼) í¬í•¨
        - ì›ë¬¸ êµ¬ì¡° ìœ ì§€
        
        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            raw_context: ì›ë³¸ RAG ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            str: ìš”ì•½ëœ ì»¨í…ìŠ¤íŠ¸ (ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì¼ë¶€ ë°˜í™˜)
        """
        self._dbg(f"[CONDENSE] ì‹œì‘ - ì›ë³¸ ê¸¸ì´: {len(raw_context)}ì, ì§ˆë¬¸: {user_question[:50]}...")
        
        def _sanitize_text(txt: str) -> str:
            """ì œì–´ë¬¸ì ì œê±° ë° íƒœê·¸ ì¶©ëŒ ë°©ì§€"""
            if not isinstance(txt, str):
                txt = str(txt)
            # ê°„ë‹¨ ì œì–´ë¬¸ì í•„í„°ë§ (LF, TAB ì œì™¸)
            txt = ''.join(ch for ch in txt if ch in ('\n', '\t') or (ord(ch) >= 32 and ch != 127))
            # íƒœê·¸ ì¡°ê¸° ì¢…ë£Œ ë°©ì§€
            txt = txt.replace("</ê¸°ì–µê²€ìƒ‰>", "[/ê¸°ì–µê²€ìƒ‰]")
            # ê³¼ë„í•œ ê¸¸ì´ í´ë¨í”„
            max_len = 30000
            return txt[:max_len]

        sanitized_rag = _sanitize_text(raw_context)
        self._dbg(f"[CONDENSE] Sanitized ê¸¸ì´: {len(sanitized_rag)}ì")

        condense_prompt = [
            {
                "role": "system",
                "content": (
                                    f"""
                ë‹¹ì‹ ì€ ê¸´ ê·œì •/ì„¸ì¹™ ë¬¸ì„œ ë¬¶ìŒì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ë¶€ë¶„ì„ "ë„“ì€ ë§¥ë½"ìœ¼ë¡œ ì¶”ì¶œÂ·í‘œì‹œí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ê·œì¹™(ë„“ì€ ë§¥ë½ í¬í•¨):
                1) ì›ë¬¸ ì „ì²´ëŠ” <ê¸°ì–µê²€ìƒ‰> íƒœê·¸ ì•ˆì— ìˆìŠµë‹ˆë‹¤.
                2) ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ê·¼ê±°ëŠ” <ë°˜ì˜>...</ë°˜ì˜> íƒœê·¸ ì•ˆì— ë‹´ë˜, ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”.
                - í‘œ/ëª©ë¡/ë²ˆí˜¸ ì¡°í•­ì€ í•´ë‹¹ í•­ëª©ì˜ ë¨¸ë¦¬ê¸€(ì œëª©/í—¤ë”)ê³¼ ì¸ì ‘ í–‰Â·í•­ê¹Œì§€ í•¨ê»˜ í¬í•¨(ìµœì†Œ Â±5~10ì¤„ ë§¥ë½).
                - "ì£¼)" í˜•íƒœì˜ ì£¼ì„/ë¹„ê³ ê°€ ë¶™ì€ ê²½ìš° í•´ë‹¹ ì£¼ì„ ì „ë¶€ í¬í•¨.
                - í•™ì Â·ê³¼ëª©Â·ë°°ë¶„ì˜ì—­Â·íŠ¸ë™ê³¼ ê°™ì€ ìˆ«ì/í•­ëª©ì€ í‘œì˜ ì—´ ë¨¸ë¦¬ë§ê³¼ ê°™ì´ í¬í•¨(í—¤ë”+í–‰ ì„¸íŠ¸).
                3) ì‚¬ìš©ìê°€ íŠ¹ì • ë²ˆí˜¸(ì˜ˆ: 1ë²ˆ, 2ë²ˆ)ë¥¼ ì–¸ê¸‰í–ˆì§€ë§Œ ëª¨í˜¸í•  ê²½ìš°, í›„ë³´ ë²ˆí˜¸ 2~3ê°œë¥¼ ëª¨ë‘ í¬í•¨í•˜ë˜ ê° ë¸”ë¡ ì•ì— [í›„ë³´] í‘œê¸°.
                4) ê´€ë ¨ ê·¼ê±°ê°€ ì¶©ë¶„ì¹˜ ì•Šë‹¤ê³  íŒë‹¨ë˜ë©´, ìƒìœ„ ë‹¨ë½(ì¡°/í•­/í‘œ ì œëª©) ë‹¨ìœ„ê¹Œì§€ í™•ì¥í•˜ì—¬ ìµœì†Œ 15ì¤„ ì´ìƒì„ ë‹´ê³ , ì§€ë‚˜ì¹œ ìš”ì•½ì„ í”¼í•˜ì„¸ìš”.
                5) ì›ë¬¸ êµ¬ì¡°(ì¡°/í•­/í˜¸/í‘œ ì œëª©)ëŠ” ìœ ì§€í•˜ê³  ì„ì˜ ì¬ì‘ì„± ê¸ˆì§€. ë°˜ë“œì‹œ ì›ë¬¸ì„ ê±°ì˜ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”.
                6) ì›ë¬¸ ë°– ì¶”ë¡ /ì°½ì‘ ê¸ˆì§€.

                ì‚¬ìš©ì ì§ˆë¬¸: {user_question}
                <ê¸°ì–µê²€ìƒ‰>{sanitized_rag}</ê¸°ì–µê²€ìƒ‰>
                """
                ),
            }
        ]
        
        self._dbg("[CONDENSE] 1ì°¨ ìš”ì•½ ì‹œë„ ì¤‘...")

        try:
            condensed = client.responses.create(
                model=self.model,
                input=condense_prompt,
                text={"format": {"type": "text"}},
            ).output_text.strip()
            
            self._dbg(f"[CONDENSE] 1ì°¨ ê²°ê³¼ - ê¸¸ì´: {len(condensed)}ì, ì¤„ ìˆ˜: {condensed.count(chr(10))}ì¤„")
            
            # ê²°ê³¼ê°€ ì§€ë‚˜ì¹˜ê²Œ ì§§ìœ¼ë©´(ì¤„ ìˆ˜<15 ë˜ëŠ” ê¸¸ì´<1000ì) ë„“ì€ ë§¥ë½ ì¬ì‹œë„
            if (condensed.count("\n") < 15) or (len(condensed) < 1000):
                self._dbg("[CONDENSE] 1ì°¨ ê²°ê³¼ ë„ˆë¬´ ì§§ìŒ -> 2ì°¨ ì‹œë„ (ë„“ì€ ë§¥ë½)")
                broader_prompt = [
                    {
                        "role": "system",
                        "content": (
                                                    f"""
                        ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‘œ/ë²ˆí˜¸ì¡°í•­/ì£¼ì„ì˜ ì „ì²´ ë§¥ë½ì„ ë„“ê²Œ í¬í•¨í•´ ì¶”ì¶œí•©ë‹ˆë‹¤.
                        ë°˜ë“œì‹œ ë‹¤ìŒì„ ì§€í‚¤ì„¸ìš”:
                        - <ë°˜ì˜>...</ë°˜ì˜> ì•ˆì— í—¤ë”(í‘œ ì œëª©/ì—´ ë¨¸ë¦¬ë§) + ê´€ë ¨ í–‰/í•­ ì „ë¶€ì™€ í•´ë‹¹ ì£¼ì„(ì£¼)ê¹Œì§€ í¬í•¨.
                        - ìµœì†Œ 25ì¤„ ì´ìƒ, ê°€ëŠ¥í•˜ë©´ ê´€ë ¨ ë¸”ë¡ì„ í†µì§¸ë¡œ í¬í•¨(ë¶ˆí•„ìš”í•œ ìš”ì•½ ê¸ˆì§€).
                        - ëª¨í˜¸í•˜ë©´ í›„ë³´ ë¸”ë¡ 2~3ê°œë¥¼ [í›„ë³´]ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë‘ í¬í•¨.
                        ì›ë¬¸: <ê¸°ì–µê²€ìƒ‰>{sanitized_rag}</ê¸°ì–µê²€ìƒ‰>
                        ì§ˆë¬¸: {user_question}
                        """
                        ),
                    }
                ]
                try:
                    self._dbg("[CONDENSE] 2ì°¨ ìš”ì•½ ì‹œë„ ì¤‘...")
                    condensed2 = client.responses.create(
                        model=self.model,
                        input=broader_prompt,
                        text={"format": {"type": "text"}},
                    ).output_text.strip()
                    
                    self._dbg(f"[CONDENSE] 2ì°¨ ê²°ê³¼ - ê¸¸ì´: {len(condensed2)}ì, ì¤„ ìˆ˜: {condensed2.count(chr(10))}ì¤„")
                    
                    # ë” ê¸¸ê³  í’ë¶€í•˜ë©´ êµì²´
                    if (condensed2.count("\n") >= condensed.count("\n")) and (len(condensed2) > len(condensed)):
                        self._dbg("[CONDENSE] 2ì°¨ ê²°ê³¼ê°€ ë” í’ë¶€í•¨ -> êµì²´")
                        condensed = condensed2
                    else:
                        self._dbg("[CONDENSE] 1ì°¨ ê²°ê³¼ ìœ ì§€")
                except Exception as e2:
                    self._dbg(f"[CONDENSE] 2ì°¨ ì‹œë„ ì‹¤íŒ¨: {e2}")
                    
            self._dbg(f"[CONDENSE] ìµœì¢… ê²°ê³¼ - ê¸¸ì´: {len(condensed)}ì")
            return condensed
            
        except Exception as e:
            # ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ì„ ì§§ê²Œ ì˜ë¼ ì‚¬ìš©
            self._dbg(f"[CONDENSE] ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {e}")
            fallback = sanitized_rag[:6000]
            self._dbg(f"[CONDENSE] Fallback ì‚¬ìš© - ê¸¸ì´: {len(fallback)}ì")
            return fallback

    def _build_final_context(
        self,
        message: str,
        condensed_rag: Optional[str],
        func_results: List[FunctionCallMetadata],
    ) -> List[Dict[str, str]]:
        """ìµœì¢… LLM ì…ë ¥ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

        ì‚¬ìš©ì ì§ˆë¬¸, ìš”ì•½ëœ ê¸°ì–µê²€ìƒ‰ ê²°ê³¼, í•¨ìˆ˜ í˜¸ì¶œ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬
        ë‹¨ì¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í˜•íƒœë¡œ ì •ë¦¬í•œ ë’¤ ê¸°ì¡´ ëŒ€í™”ë¬¸ë§¥ì— ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            message: í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸.
            condensed_rag: LLMìœ¼ë¡œ ê°€ê³µëœ ê¸°ì–µê²€ìƒ‰ ìš”ì•½ ë¬¸ìì—´. ì—†ìœ¼ë©´ None.
            func_results: í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ëª©ë¡.

        Returns:
            OpenAI Responses APIì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸.
        """

        base_context = self.to_openai_context(self.context[:])
        has_rag = bool(condensed_rag and condensed_rag.strip())
        has_funcs = bool(func_results)

        sections: List[str] = []

        # 1) ì‚¬ìš©ì ì¿¼ë¦¬ ì§€ì¹¨
        query_guidance = (
            f"ì´ê²ƒì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤: {message}\n"
            "ë‹¤ìŒ ì •ë³´ë¥¼ [ì‚¬ìš©ìì¿¼ë¦¬ì§€ì¹¨],[ì¼ë°˜ì§€ì¹¨],[ê¸°ì–µê²€ìƒ‰ì§€ì¹¨],[ì›¹ê²€ìƒ‰ì§€ì¹¨] ë“± ì„œìˆ ëœ ì„œìˆ ê³¼ ì§€ì¹¨ì— ë”°ë¼ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë‹µì— ë§ê²Œ í†µí•©í•´ ì „ë‹¬í•˜ì„¸ìš”.\n"
            "- í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜\n"
            "- ê¸°ì–µê²€ìƒ‰ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜ / í•¨ìˆ˜ í˜¸ì¶œ ì¡´ì¬ ìì²´ëŠ” ì–¸ê¸‰ ê¸ˆì§€"
        )
        sections.append("[ì‚¬ìš©ìì¿¼ë¦¬ì§€ì¹¨]\n" + query_guidance)

        # 2) ì¼ë°˜ ì§€ì¹¨
        sections.append("[ì¼ë°˜ì§€ì¹¨]\n" + self.instruction)

        # 3) ê¸°ì–µê²€ìƒ‰ ì§€ì¹¨ ë° ë³¸ë¬¸
        if has_rag:
            rag_guidance = (
                "ê¸°ì–µê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. <ë°˜ì˜> </ë°˜ì˜> íƒœê·¸ ë‚´ë¶€ ë‚´ìš©ì„ ë³´ê³  ì‚¬ìš©ìì˜ ì›í•˜ëŠ” ì¿¼ë¦¬ì— ë§ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”. "
                "<ê¸°ì–µê²€ìƒ‰></ê¸°ì–µê²€ìƒ‰> íƒœê·¸ëŠ” ì°¸ì¡°ìš©ì´ë©° íƒœê·¸ ë°– ì„ì˜ ì°½ì‘ ê¸ˆì§€"
            )
            sections.append("[ê¸°ì–µê²€ìƒ‰ì§€ì¹¨]\n" + rag_guidance)
            sections.append("[ê¸°ì–µê²€ìƒ‰]\n<ê¸°ì–µê²€ìƒ‰>\n" + condensed_rag + "\n</ê¸°ì–µê²€ìƒ‰>")

        web_status: Optional[str] = None

        if has_funcs:
            formatted_blocks_web: List[str] = []
            formatted_blocks_other: List[str] = []
            web_outputs: List[str] = []

            for meta in func_results:
                args_str: str
                try:
                    args_str = json.dumps(meta.arguments, ensure_ascii=False)
                except (TypeError, ValueError):
                    args_str = str(meta.arguments)

                out_text = meta.output if isinstance(meta.output, str) else str(meta.output)
                max_len = 4000
                if len(out_text) > max_len:
                    out_text = out_text[:max_len] + "...<truncated>"

                block = f"<function name='{meta.name}' args='{args_str}'>\n{out_text}\n</function>"

                if meta.name == "search_internet":
                    web_outputs.append(out_text)
                    formatted_blocks_web.append(block)
                else:
                    formatted_blocks_other.append(block)

            web_functions_block = "\n".join(formatted_blocks_web) if formatted_blocks_web else ""
            other_functions_block = "\n".join(formatted_blocks_other) if formatted_blocks_other else ""

            if not web_outputs:
                web_status = "not-run"
            else:
                def _is_error_or_empty(txt: str) -> bool:
                    t = (txt or "").strip()
                    if not t:
                        return True
                    err_keywords = [
                        "ğŸš¨",
                        "âŒ",
                        "ì˜¤ë¥˜",
                        "error",
                        "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜",
                        "no result",
                        "did_call=False",
                    ]
                    return any(k.lower() in t.lower() for k in err_keywords)

                all_bad = all(_is_error_or_empty(t) for t in web_outputs)
                web_status = "empty-or-error" if all_bad else "ok"

            web_guidance = (
                "ë‹¤ìŒì€ ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼ì…ë‹ˆë‹¤. ê³µì‹ ê·¼ê±°ê°€ ì•„ë‹ˆë¯€ë¡œ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”. "
                "ê²€ìƒ‰ì´ ì•ˆë˜ì–´ ìš°íšŒ/ë¬¸ì˜ ì•ˆë‚´ë§Œ ìˆì„ ê²½ìš°, ë¬´ì‹œí•˜ê³  ì´ ë‚´ìš©ì€'ì°¸ì¡°ë§Œ' í•˜ì„¸ìš”. ë°˜ë“œì‹œ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°ë¥¼ ìš°ì„  ë°˜ì˜í•˜ì„¸ìš”. ì°¸ì¡°ë€ ì•ˆë‚´ ì „í™”ë²ˆí˜¸ ì‚¬ì´íŠ¸ë§Œì„ ë°˜ì˜í•˜ëŠ”ê²ƒì„ ë§í•©ë‹ˆë‹¤ "
            )
            sections.append("[ì›¹ê²€ìƒ‰ì§€ì¹¨]\n" + web_guidance)
            if web_functions_block:
                sections.append("[ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼]\n<ì¸í„°ë„·ê²€ìƒ‰>\n" + web_functions_block + "\n</ì¸í„°ë„·ê²€ìƒ‰>")

            func_guidance = (
                "ë‹¤ìŒì€ í•¨ìˆ˜(ê²€ìƒ‰/ë©”ë‰´ ë“±) í˜¸ì¶œ ê²°ê³¼ì…ë‹ˆë‹¤. <í•¨ìˆ˜ê²°ê³¼> íƒœê·¸ ë‚´ë¶€ ë‚´ìš©ì€ ì°¸ê³ ìš©ì´ë©°, ë°˜ë“œì‹œ ì•„ë˜ ê¸°ì–µê²€ìƒ‰(<ê¸°ì–µê²€ìƒ‰>) ê·¼ê±°ë¥¼ ìš°ì„  ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”. "
                "'í•¨ìˆ˜ í˜¸ì¶œ'ì´ë¼ëŠ” í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ê³ , ê±°ì§“ ì •ë³´ ìƒì„± ê¸ˆì§€."
            )
            sections.append("[í•¨ìˆ˜ê²°ê³¼ì§€ì¹¨]\n" + func_guidance)
            if other_functions_block:
                sections.append("[í•¨ìˆ˜ê²°ê³¼]\n<í•¨ìˆ˜ê²°ê³¼>\n" + other_functions_block + "\n</í•¨ìˆ˜ê²°ê³¼>")

            if web_status and web_status != "not-run":
                status_kr = {
                    "ok": "ì •ìƒ",
                    "empty-or-error": "ê²°ê³¼ì—†ìŒ/ì˜¤ë¥˜",
                    "not-run": "ì‹¤í–‰ì•ˆí•¨",
                }[web_status]
                sections.append("[ì›¹ê²€ìƒ‰ìƒíƒœ]\n" + status_kr)

            if web_status in ("empty-or-error", "not-run"):
                if has_rag:
                    sections.append(
                        "[ì›¹ê²€ìƒ‰ê²°ê³¼ì—†ìŒì§€ì¹¨]\nì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì…ë‹ˆë‹¤. ê³µì‹ ê·œì •ì€ ì•„ë˜ ê¸°ì–µê²€ìƒ‰(<ê¸°ì–µê²€ìƒ‰>) ê·¼ê±°ë¥¼ ë°˜ë“œì‹œ ìš°ì„  í™•ì¸í•˜ì„¸ìš”. ê²€ìƒ‰ì´ ë˜ì§€ ì•Šê±°ë‚˜ ë¬¸ì˜ ì•ˆë‚´ë§Œ ìˆì„ ê²½ìš°, í•´ë‹¹ ë‚´ìš©ì€ ì°¸ê³ ë§Œ í•˜ì‹œê³  ë°˜ë“œì‹œ ì•„ë˜ ê·œì • ê·¼ê±°ë¥¼ ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”."
                    )
                else:
                    sections.append(
                        "[ì›¹ê²€ìƒ‰ê²°ê³¼ì—†ìŒì§€ì¹¨]\nì›¹ê²€ìƒ‰ ê²°ê³¼ëŠ” ì—†ì—ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŒì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ì•Œë¦¬ê³ , í•„ìš”í•œ ì¶”ê°€ ì •ë³´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì²­í•˜ì„¸ìš”."
                    )

        if has_rag and has_funcs:
            extra_note = ""
            if web_status == "empty-or-error":
                extra_note = " ì›¹ê²€ìƒ‰ì´ ê²°ê³¼ì—†ìŒ/ì˜¤ë¥˜ì—¬ë„ ê¸°ì–µê²€ìƒ‰ì´ ì¡´ì¬í•˜ë©´ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  í•˜ì§€ ë§ê³  ê¸°ì–µê²€ìƒ‰ ê·¼ê±°ë¡œ ë‹µí•  ê²ƒ."
            merge_instruction = (
                "ìœ„ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°(<ê¸°ì–µê²€ìƒ‰>)ì™€ ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼(<ì¸í„°ë„·ê²€ìƒ‰>), ê¸°íƒ€ í•¨ìˆ˜ê²°ê³¼(<í•¨ìˆ˜ê²°ê³¼>)ë¥¼ ëŒ€ì¡°í•˜ì—¬ ëª¨ìˆœ ì—†ì´ ë‹µí•˜ì„¸ìš”. "
                "í•µì‹¬ ë‹µ ë¨¼ì € ì œì‹œí•˜ê³ , í•„ìš”í•œ ê·¼ê±°ë§Œ ì¶•ì•½ ì¸ìš©. ì¸í„°ë„· ê²€ìƒ‰ê²°ê³¼ëŠ” ì°¸ê³ ìš©ì´ë©°, ìš°íšŒ/ë¬¸ì˜ ì•ˆë‚´ë§Œ ìˆì„ ê²½ìš° 'ì°¸ì¡°ë§Œ' í•˜ê³  -ì°¸ì¡°ë€ ì•ˆë‚´ ì „í™”ë²ˆí˜¸ ì‚¬ì´íŠ¸ë§Œì„ ë°˜ì˜í•˜ëŠ”ê²ƒì„ ë§í•©ë‹ˆë‹¤   "
                "ë°˜ë“œì‹œ ê¸°ì–µê²€ìƒ‰ ê·¼ê±°ë¥¼ ìš°ì„  ë°˜ì˜í•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…ì‹œ." + extra_note
            )
            sections.append("[í†µí•©ì§€ì¹¨]\n" + merge_instruction)

        # ì¶”ê°€ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
        if not (has_rag or has_funcs):
            self._last_web_status = web_status
            return base_context

        base_context.append({
            "role": "system",
            "content": "\n\n".join(sections),
        })

        self._last_web_status = web_status
        return base_context

    async def _analyze_and_execute_functions(
        self, 
        message: str
    ) -> List[FunctionCallMetadata]:
        """í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„ ë° ì‹¤í–‰
        
        1) FunctionCalling.analyze()ë¡œ í•„ìš”í•œ í•¨ìˆ˜ íŒŒì•…
        2) ê° í•¨ìˆ˜ ì‹¤í–‰
        3) í•™ì‹ í‚¤ì›Œë“œ ê¸°ë°˜ fallback í˜¸ì¶œ(ê·œì¹™ ê¸°ë°˜) í¬í•¨
        4) ê²°ê³¼ë¥¼ FunctionCallMetadata ë¦¬ìŠ¤íŠ¸ë¡œ ì§ë ¬í™”í•˜ì—¬ ë°˜í™˜
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            
        Returns:
            í•¨ìˆ˜ í˜¸ì¶œ ë©”íƒ€ë°ì´í„° ëª©ë¡
        """
        func_results: List[FunctionCallMetadata] = []
        
        # 1) í•¨ìˆ˜ ë¶„ì„ ë° ì‹¤í–‰
        analyzed = self.func_calling.analyze(message, self.tools)
        
        for tool_call in analyzed:
            if getattr(tool_call, "type", None) != "function_call":
                continue
                
            func_name = tool_call.name
            call_id = getattr(tool_call, "call_id", "call_unknown")
            
            # ì¸ì íŒŒì‹±
            try:
                func_args = json.loads(tool_call.arguments)
            except (json.JSONDecodeError, TypeError) as e:
                self._dbg(f"[FUNCTION] ì¸ì íŒŒì‹± ì‹¤íŒ¨: {func_name}, {e}")
                continue
                
            if not isinstance(func_args, dict):
                self._dbg(f"[FUNCTION] ì˜ëª»ëœ ì¸ì í˜•ì‹: {func_name}")
                continue
            
            # í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤ ì°¾ê¸°
            func = self.available_functions.get(func_name)
            if not func:
                self._dbg(f"[FUNCTION] ë¯¸ë“±ë¡ í•¨ìˆ˜: {func_name}")
                continue
            
            # í•¨ìˆ˜ ì‹¤í–‰
            try:
                # ì•ˆì „ ê¸°ë³¸ê°’ ë³´ê°•
                if func_name == "search_internet":
                    func_args.setdefault("chat_context", self.context[:])
                elif func_name == "get_halla_cafeteria_menu":
                    func_args.setdefault("date", "ì˜¤ëŠ˜")
                    # mealì€ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì „ì²´ ë¼ë‹ˆ ë°˜í™˜
                
                output = func(**func_args)
                
                # ë©”íƒ€ë°ì´í„° ìƒì„±
                func_results.append(FunctionCallMetadata(
                    name=func_name,
                    arguments=func_args,
                    output=str(output),
                    call_id=call_id,
                    is_fallback=False
                ))
                
            except Exception as exc:
                self._dbg(f"[FUNCTION] {func_name} ì‹¤í–‰ ì‹¤íŒ¨: {exc}")
                # ì‹¤íŒ¨í•´ë„ ë©”íƒ€ë°ì´í„°ëŠ” ê¸°ë¡ (ì—ëŸ¬ ë©”ì‹œì§€ í¬í•¨)
                func_results.append(FunctionCallMetadata(
                    name=func_name,
                    arguments=func_args,
                    output=f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {str(exc)}",
                    call_id=call_id,
                    is_fallback=False
                ))
        
        # 2) í•™ì‹ ë³´ê°• Fallback
        lowered = message.lower()
        cafeteria_keywords = any(k in lowered for k in ["í•™ì‹", "ì‹ë‹¨", "ì ì‹¬", "ì €ë…", "ë©”ë‰´", "ì¡°ì‹", "ì„ì‹", "ì•„ì¹¨", "ì˜¤ëŠ˜ ë©”ë‰´", "ë°¥ ë­"])
        already_called_cafeteria = any(meta.name == "get_halla_cafeteria_menu" for meta in func_results)
        
        if cafeteria_keywords and not already_called_cafeteria:
            try:
                self._dbg("[FUNCTION] Cafeteria fallback engaged")
                
                # ë¼ë‹ˆ ì¶”ì¶œ
                meal_pref = None
                if any(x in lowered for x in ["ì¡°ì‹", "ì•„ì¹¨"]):
                    meal_pref = "ì¡°ì‹"
                elif any(x in lowered for x in ["ì„ì‹", "ì €ë…"]):
                    meal_pref = "ì„ì‹"
                elif "ì ì‹¬" in lowered or "ì¤‘ì‹" in lowered:
                    meal_pref = "ì¤‘ì‹"
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_pref = "ì˜¤ëŠ˜"
                if "ë‚´ì¼" in lowered:
                    date_pref = "ë‚´ì¼"
                else:
                    import re
                    m = re.search(r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})", message)
                    if m:
                        date_pref = m.group(1)
                
                caf_args = {"date": date_pref, "meal": meal_pref}
                get_cafeteria_fn = self.available_functions.get("get_halla_cafeteria_menu")
                
                if not get_cafeteria_fn:
                    raise RuntimeError("get_halla_cafeteria_menu not registered")
                
                caf_out = get_cafeteria_fn(**caf_args)
                
                # Fallback ë©”íƒ€ë°ì´í„° ì¶”ê°€
                func_results.append(FunctionCallMetadata(
                    name="get_halla_cafeteria_menu",
                    arguments=caf_args,
                    output=str(caf_out),
                    call_id="cafeteria_auto",
                    is_fallback=True
                ))
                
            except Exception as e:
                self._dbg(f"[FUNCTION] í•™ì‹ fallback ì‹¤íŒ¨: {e}")
        
        return func_results

    async def _stream_openai_response(
        self, 
        context: List[Dict[str, str]]
    ):
        """OpenAI Responses API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        
        JSON Lines í˜•ì‹ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ yieldí•©ë‹ˆë‹¤.
        
        Args:
            context: OpenAI APIì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸
            
        Yields:
            Dict: ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸
                - {"type": "delta", "content": "..."}: í…ìŠ¤íŠ¸ ì²­í¬
                - {"type": "completed", "text": "..."}: ì™„ë£Œëœ ì „ì²´ í…ìŠ¤íŠ¸
        """
        completed_text = ""
        
        try:
            stream = client.responses.create(
                model=self.model,
                input=context,
                top_p=1,
                stream=True,
                text={"format": {"type": "text"}}
            )
            
            for event in stream:
                if event.type == "response.output_text.delta":
                    # ë¸íƒ€ ì´ë²¤íŠ¸ - ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì²­í¬
                    yield {
                        "type": "delta",
                        "content": event.delta
                    }
                    completed_text += event.delta
                    
                elif event.type == "response.output_item.done":
                    # ì¶œë ¥ ì•„ì´í…œ ì™„ë£Œ - ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                    item = event.item
                    if item.type == "message" and item.role == "assistant":
                        for part in item.content:
                            if getattr(part, "type", None) == "output_text":
                                completed_text = part.text
                                
                elif event.type == "response.failed":
                    # ì‹¤íŒ¨ ì´ë²¤íŠ¸
                    yield {
                        "type": "error",
                        "message": "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                    }
                    return
                    
            # ì™„ë£Œ ì´ë²¤íŠ¸
            yield {
                "type": "completed",
                "text": completed_text
            }
            
        except Exception as e:
            self._dbg(f"[STREAM] OpenAI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield {
                "type": "error",
                "message": f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"
            }

    async def stream_chat(
        self, 
        message: str, 
        language: str = "KOR"
    ) -> AsyncGenerator[str, None]:
        """ì±—ë´‡ ìŠ¤íŠ¸ë¦¬ë° í†µí•© ë©”ì„œë“œ
        
        JSON Lines í˜•ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
        
        ì²˜ë¦¬ íë¦„:
        1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”
        2. ì–¸ì–´ë³„ ì§€ì¹¨ ì¶”ê°€
        3. RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° ìš”ì•½
        4. í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„/ì‹¤í–‰
        5. ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        6. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        7. ë©”íƒ€ë°ì´í„° ì „ì†¡
        8. ì™„ë£Œ ì‹ í˜¸
        9. ì‘ë‹µ ì €ì¥
        
        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            language: ì‘ë‹µ ì–¸ì–´ (KOR, ENG, VI, JPN, CHN, UZB, MNG, IDN)
            
        Yields:
            JSON Lines í˜•ì‹ì˜ ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸
        """
        self._dbg(f"[STREAM_CHAT] ì‹œì‘ - ë©”ì‹œì§€: {message[:50]}..., ì–¸ì–´: {language}")
        
        # === 1ë‹¨ê³„: ì´ˆê¸°í™” ===
        self.add_user_message_in_context(message)
        metadata = ChatMetadata()
        self._dbg("[STREAM_CHAT] 1ë‹¨ê³„: ë©”ì‹œì§€ ì¶”ê°€ ì™„ë£Œ")
        
        # === 2ë‹¨ê³„: ì–¸ì–´ë³„ ì§€ì¹¨ ì¶”ê°€ ===
        language_instruction = self._get_language_instruction(language)
        self.context[-1]["content"] += " " + language_instruction
        self._dbg(f"[STREAM_CHAT] 2ë‹¨ê³„: ì–¸ì–´ ì§€ì¹¨ ì¶”ê°€ ì™„ë£Œ - {language}")
        
        # === 3ë‹¨ê³„: RAG ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ===
        self._dbg("[STREAM_CHAT] 3ë‹¨ê³„: RAG ê²€ìƒ‰ ì‹œì‘...")
        rag_result = self.rag_service.retrieve_context(message)
        condensed_rag = None
        
        if rag_result.merged_documents_text:
            self._dbg(f"[STREAM_CHAT] RAG ê²€ìƒ‰ ì™„ë£Œ - ì›ë³¸ ê¸¸ì´: {len(rag_result.merged_documents_text)}ì")
            
            # === RAG ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ë””ë²„ê·¸ ì¶œë ¥ ===
            self._dbg("=" * 80)
            self._dbg("[RAG ë””ë²„ê·¸] ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì •ë³´:")
            self._dbg(f"  - ê·œì • ì—¬ë¶€: {rag_result.is_regulation}")
            self._dbg(f"  - íŒë‹¨ ì´ìœ : {rag_result.gate_reason or 'N/A'}")
            self._dbg(f"  - ê²€ìƒ‰ ì†ŒìŠ¤: {rag_result.context_source}")
            self._dbg(f"  - ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(rag_result.hits)}")
            self._dbg(f"  - MongoDB ë¬¸ì„œ: {rag_result.document_count}ê°œ")
            self._dbg(f"  - í”„ë¦¬ë·° ë¬¸ì„œ: {rag_result.preview_count}ê°œ")
            self._dbg(f"  - ì²­í¬ ID ìˆ˜: {len(rag_result.chunk_ids)}ê°œ")
            #ê²€ìƒ‰ë¬¸ì„œ id ìƒ˜í”Œ ì¶œë ¥ (ìµœëŒ€ 5ê°œ) 5ê°œ ì´ˆê³¼ ì‹œ ìƒëµí‘œì‹œ
            if rag_result.chunk_ids:
                chunk_ids_str = ", ".join(rag_result.chunk_ids[:5])
                if len(rag_result.chunk_ids) > 5:
                    chunk_ids_str += f" ... (ì™¸ {len(rag_result.chunk_ids) - 5}ê°œ)"
                self._dbg(f"  - ì²­í¬ ID ìƒ˜í”Œ: [{chunk_ids_str}]")
            
            # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì¶œë ¥ (ì²˜ìŒ 500ì)
            context_sample = rag_result.merged_documents_text[:500]
            if len(rag_result.merged_documents_text) > 500:
                context_sample += "..."
            self._dbg(f"  - ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒ˜í”Œ:\n{context_sample}")
            self._dbg("=" * 80)
            
            condensed_rag = await self._condense_rag_context(
                message, rag_result.merged_documents_text
            )
            self._dbg(f"[STREAM_CHAT] RAG ìš”ì•½ ì™„ë£Œ - ìš”ì•½ ê¸¸ì´: {len(condensed_rag)}ì")
            metadata.rag = RagMetadata(
                is_regulation=rag_result.is_regulation,
                gate_reason=rag_result.gate_reason or "",
                context_source=rag_result.context_source,
                hits_count=len(rag_result.hits),
                document_count=rag_result.document_count,
                preview_count=rag_result.preview_count,
                chunk_ids=list(rag_result.chunk_ids),
                source_documents=rag_result.source_documents,  # ì¶œì²˜ ë¬¸ì„œ ì •ë³´ ì¶”ê°€
                raw_context=rag_result.merged_documents_text,  # ì›ë³¸ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
                condensed_context=condensed_rag,
            )
        else:
            self._dbg("[STREAM_CHAT] RAG ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        
        # === 4ë‹¨ê³„: í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„/ì‹¤í–‰ ===
        self._dbg("[STREAM_CHAT] 4ë‹¨ê³„: í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„/ì‹¤í–‰ ì‹œì‘...")
        func_results = await self._analyze_and_execute_functions(message)
        metadata.functions = func_results
        self._dbg(f"[STREAM_CHAT] í•¨ìˆ˜ í˜¸ì¶œ ì™„ë£Œ - {len(func_results)}ê°œ í•¨ìˆ˜ ì‹¤í–‰")
        
        # ì›¹ê²€ìƒ‰ ìƒíƒœ ì„¤ì •
        web_search_used = any(meta.name == "search_internet" for meta in func_results)
        if web_search_used:
            # ì›¹ê²€ìƒ‰ ê²°ê³¼ ì˜¤ë¥˜ í™•ì¸
            web_meta = next((m for m in func_results if m.name == "search_internet"), None)
            if web_meta and any(k in web_meta.output.lower() for k in ["ì˜¤ë¥˜", "error", "âŒ"]):
                metadata.web_search_status = "empty-or-error"
            else:
                metadata.web_search_status = "ok"
        else:
            metadata.web_search_status = "not-run"
        
        # === 5ë‹¨ê³„: ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ===
        final_context = self._build_final_context(
            message=message,
            condensed_rag=condensed_rag,
            func_results=func_results,
        )
        
        # === 6ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ===
        completed_text = ""
        async for chunk in self._stream_openai_response(final_context):
            if chunk["type"] == "delta":
                yield json.dumps(chunk, ensure_ascii=False) + "\n"
            elif chunk["type"] == "completed":
                completed_text = chunk["text"]
            elif chunk["type"] == "error":
                yield json.dumps(chunk, ensure_ascii=False) + "\n"
                return
        
        # === 7ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì „ì†¡ ===
        self._dbg("[STREAM_CHAT] 7ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì „ì†¡")
        yield json.dumps({
            "type": "metadata",
            "data": metadata.to_dict()
        }, ensure_ascii=False) + "\n"
        
        # === 8ë‹¨ê³„: ì™„ë£Œ ì‹ í˜¸ ===
        self._dbg("[STREAM_CHAT] 8ë‹¨ê³„: ì™„ë£Œ ì‹ í˜¸ ì „ì†¡")
        yield json.dumps({"type": "done"}, ensure_ascii=False) + "\n"
        
        # === 9ë‹¨ê³„: ì‘ë‹µ ì €ì¥ ===
        self.add_response_stream(completed_text)
        self._dbg(f"[STREAM_CHAT] 9ë‹¨ê³„: ì‘ë‹µ ì €ì¥ ì™„ë£Œ - ê¸¸ì´: {len(completed_text)}ì")
        self._dbg("[STREAM_CHAT] ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")

'''
if __name__ == "__main__":
    #ì‹¤í–‰íë¦„
    ë‹¨ê³„	ë‚´ìš©
#1ï¸âƒ£	ì‚¬ìš©ì ì…ë ¥ ë°›ìŒ (user_input)
#2ï¸âƒ£	â†’ add_user_message_in_context() ë¡œ user ë©”ì‹œì§€ë¥¼ ë¬¸ë§¥ì— ì¶”ê°€
#3ï¸âƒ£	â†’ analyze() ë¡œ í•¨ìˆ˜ í˜¸ì¶œì´ í•„ìš”í•œì§€ íŒë‹¨
#4ï¸âƒ£	â†’ í•„ìš”í•˜ë©´ í•¨ìˆ˜ ì‹¤í–‰ + ê²°ê³¼ë¥¼ temp_contextì— ì¶”ê°€
#5ï¸âƒ£	â†’ chatbot._send_request_Stream(temp_context) ë¡œ ì‘ë‹µ ë°›ìŒ
#6ï¸âƒ£	âœ… streamed_response ê²°ê³¼ë¥¼ ì§ì ‘ add_response_stream()ìœ¼ë¡œ ìˆ˜ë™ ì €ì¥
    system_role = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ì±—ë´‡ì…ë‹ˆë‹¤."
    instruction = "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ê³ , í•„ìš”í•œ ê²½ìš° í•¨ìˆ˜ í˜¸ì¶œì„ í†µí•´ ì¶”ê°€ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
    # ChatbotStream ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    chatbot = ChatbotStream(
        model.advanced,
        system_role=system_role,
        instruction=instruction,
        user="ëŒ€ê¸°",
        assistant="memmo")
    func_calling=FunctionCalling(model.advanced)
    print("===== Chatbot Started =====")
    print("ì´ˆê¸° context:", chatbot.context)
    print("ì‚¬ìš©ìê°€ 'exit'ë¼ê³  ì…ë ¥í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
    
   # ì¶œë ¥: {}
    

    while True:
        try:
            user_input = input("User > ")

            if user_input.strip().lower() == "exit":
                print("Chatbot ì¢…ë£Œ.")
                break

            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            chatbot.add_user_message_in_context(user_input)

            # 1) ê¸°ì–µê²€ìƒ‰ í›„ë³´ ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            rag_ctx = chatbot.get_rag_context(user_input)
            has_rag = rag_ctx is not None and len(rag_ctx.strip()) > 0

            # 2) í•¨ìˆ˜ í˜¸ì¶œ ë¶„ì„ ë° ì‹¤í–‰
            analyzed = func_calling.analyze(user_input, tools)
            func_msgs = []  # function_call + function_call_output ë©”ì‹œì§€ ëˆ„ì 
            func_outputs = []  # í•¨ìˆ˜ ê²°ê³¼ ë¬¸ìì—´ ëˆ„ì 

            for tool_call in analyzed:  # analyzedëŠ” list of function_call objects
                if getattr(tool_call, "type", None) != "function_call":
                    continue

                func_name = tool_call.name
                func_args = json.loads(tool_call.arguments)
                call_id = tool_call.call_id

                func_to_call = func_calling.available_functions.get(func_name)
                if not func_to_call:
                    print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
                    continue

                try:
                    # ì•ˆì „ ê¸°ë³¸ê°’ ë³´ê°•: ë¶„ì„ê¸°ê°€ ì¼ë¶€ ì¸ìë¥¼ ìƒëµí•´ë„ ë™ì‘í•˜ë„ë¡
                    if func_name == "get_halla_cafeteria_menu":
                        func_args.setdefault("date", "ì˜¤ëŠ˜")
                        func_args.setdefault("meal", "ì¤‘ì‹")
                    func_response = (
                        func_to_call(chat_context=chatbot.context[:], **func_args)
                        if func_name == "search_internet"
                        else func_to_call(**func_args)
                    )

                    # function_call/ output ë©”ì‹œì§€ êµ¬ì„±
                    func_msgs.extend([
                        {
                            "type": "function_call",
                            "call_id": call_id,
                            "name": func_name,
                            "arguments": tool_call.arguments,
                        },
                        {
                            "type": "function_call_output",
                            "call_id": call_id,
                            "output": str(func_response),
                        },
                    ])
                    func_outputs.append(str(func_response))
                except Exception as e:
                    print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

            has_funcs = len(func_outputs) > 0

            # ë³´ê°•: í•™ì‹/ì‹ë‹¨ ì§ˆì˜ì¼ ê²½ìš°, ë¶„ì„ê¸°ê°€ í˜¸ì¶œì„ ì•ˆ í–ˆë”ë¼ë„ ì§ì ‘ í•¨ìˆ˜ í˜¸ì¶œ ì‹œë„
            lowered = user_input.lower()
            if ("í•™ì‹" in lowered) or ("ì‹ë‹¨" in lowered) or ("ì ì‹¬" in lowered) or ("ì €ë…" in lowered) or ("ë©”ë‰´" in lowered) or ("ì¡°ì‹" in lowered):
                if not has_funcs:
                    try:
                        # ê¸°ë³¸ê°’: ì˜¤ëŠ˜/ì¤‘ì‹, ê°„ë‹¨ ê·œì¹™ìœ¼ë¡œ ë¼ë‹ˆ/ë‚ ì§œ ì¶”ì¶œ
                        meal_pref = "ì¤‘ì‹"
                        if ("ì¡°ì‹" in lowered) or ("ì•„ì¹¨" in lowered):
                            meal_pref = "ì¡°ì‹"
                        elif ("ì„ì‹" in lowered) or ("ì €ë…" in lowered):
                            meal_pref = "ì„ì‹"
                        # ë‚ ì§œ í‚¤ì›Œë“œ
                        date_pref = "ì˜¤ëŠ˜"
                        if "ë‚´ì¼" in lowered:
                            date_pref = "ë‚´ì¼"
                        else:
                            import re as _re
                            m = _re.search(r"(\d{4}[./-]\d{1,2}[./-]\d{1,2})", user_input)
                            if m:
                                date_pref = m.group(1)
                        caf_args = {"date": date_pref, "meal": meal_pref}
                        from chatbotDirectory.functioncalling import get_halla_cafeteria_menu
                        caf_out = get_halla_cafeteria_menu(**caf_args)
                        # ë©”ì‹œì§€ í˜•íƒœë¡œ ì‚½ì…í•˜ì—¬ ëª¨ë¸ì´ ê·¼ê±°ë¡œ í™œìš©
                        call_id = "cafeteria_auto"
                        func_msgs.extend([
                            {
                                "type": "function_call",
                                "call_id": call_id,
                                "name": "get_halla_cafeteria_menu",
                                "arguments": json.dumps(caf_args, ensure_ascii=False),
                            },
                            {
                                "type": "function_call_output",
                                "call_id": call_id,
                                "output": str(caf_out),
                            },
                        ])
                        func_outputs.append(str(caf_out))
                        has_funcs = True
                    except Exception as e:
                        print(f"[ë³´ê°• í˜¸ì¶œ ì‹¤íŒ¨] get_halla_cafeteria_menu: {e}")

            # 3) ìµœì¢… temp_context êµ¬ì„±
            temp_context = chatbot.to_openai_context(chatbot.context[:])

            # ì „ë°˜ ì§€ì¹¨: ì‚¬ìš©ì ì¿¼ë¦¬ì™€ í†µí•© ì§€ì‹œ
            temp_context.append({
                "role": "system",
                "content": (
                    f"ì´ê²ƒì€ ì‚¬ìš©ì ì¿¼ë¦¬ì…ë‹ˆë‹¤: {user_input}\n"
                    "ë‹¤ìŒ ì •ë³´ë¥¼ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëŒ€ë‹µì— ë§ê²Œ í†µí•©í•´ ì „ë‹¬í•˜ì„¸ìš”.\n"
                    "- í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜\n- ê¸°ì–µê²€ìƒ‰ ê²°ê³¼: ìˆìœ¼ë©´ ë°˜ì˜"
                ),
            })
            # ì¼ë°˜ ì§€ì¹¨ ì¶”ê°€
            temp_context.append({"role": "system", "content": chatbot.instruction})

            if has_rag:
                # RAG ì•ˆë‚´ + ê·¼ê±° íˆ¬ì…
                temp_context.append({"role": "system", "content": "ê²€ìƒ‰ê²°ê³¼ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì›í•˜ëŠ” ì¿¼ë¦¬ì— ë§ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”."})
                temp_context.append({"role": "system", "content": f"[ê²€ìƒ‰ê²°ê³¼]\n{rag_ctx}"})

            if has_funcs:
                # í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ ì•ˆë‚´ ë° ë©”ì‹œì§€ ì‚½ì…
                temp_context.append({"role": "system", "content": "í•¨ìˆ˜í˜¸ì¶œê²°ê³¼ì…ë‹ˆë‹¤. ì´ê±¸ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë‹µì— ì‘í•˜ì„¸ìš”."})
                temp_context.extend(func_msgs)

            if has_rag and has_funcs:
                temp_context.append({
                    "role": "system",
                    "content": "ì•„ë˜ í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ì™€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ëª¨ë‘ í™œìš©í•´, ë‘ ë¬¸ë§¥ì´ ì–´ë–»ê²Œ ë„ì¶œë˜ì—ˆëŠ”ì§€ í•œ ì¤„ë¡œ ì„¤ëª…í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.",
                })

            if not has_rag and not has_funcs:
                # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì¼ë°˜ ì±—ë´‡ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì‘ë‹µ
                print("RAG/í•¨ìˆ˜í˜¸ì¶œ ê²°ê³¼ ì—†ìŒ â†’ ì¼ë°˜ ì±—ë´‡ ì‘ë‹µ")
                streamed = chatbot.send_request_Stream()
                chatbot.add_response_stream(streamed)
                print("\n===== Chatbot Context Updated =====")
                print(chatbot.context)
                continue

            # 4) ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
            streamed_response = chatbot._send_request_Stream(temp_context=temp_context)
            chatbot.add_response_stream(streamed_response)

            print("\n===== Chatbot Context Updated =====")
            print(chatbot.context)

        except KeyboardInterrupt:
            print("\nì‚¬ìš©ì ì¢…ë£Œ(Ctrl+C)")
            break
        except Exception as e:
            print(f"[ë£¨í”„ ì—ëŸ¬] {e}")
            continue

'''